# build:
#> podman build -t rocm-serve:7.1.1 -f Containerfile.rocm-7.1.1
#
# run llama-server with podman:
#  - mount simple model file:
#> podman run --rm --device /dev/dri --device /dev/kfd --privileged --network=host -v ~/LLM/openai_gpt-oss-120b/MXFP4.gguf:/models/default.gguf rocm-serve:7.1.1 --ctx-size 0 --jinja
#
#  - mount models path:
#> podman run --rm --device /dev/dri --device /dev/kfd --privileged --network=host -v ~/LLM:/models rocm-serve:7.1.1 --ctx-size 0 -m /models/openai_gpt-oss-120b/MXFP4.gguf --jinja
#
# run other llama exec:
#> podman run --rm --device /dev/dri --device /dev/kfd --privileged --network=host -v ~/LLM:/models --entrypoint="llama-bench" rocm-serve:7.1.1 -m /models/openai_gpt-oss-120b/MXFP4.gguf -b 8192 -ub 2048 -fa 1 -ngl 999 --mmap 0 -p "1,1,2,4,8,16,32,64,128,256,512,1024,2048,4096,8192"
#
#  interactive run:
#> podman run --rm -it --device /dev/dri --device /dev/kfd --privileged --network=host -v ~/LLM:/models --entrypoint="/usr/bin/bash" rocm-serve:7.1.1
#>>  llama-cli -m /models/openai_gpt-oss-120b/MXFP4.gguf --jinja --ctx-size 0
#

#------------------------------------------------------------------------------------------
# build stage
FROM registry.fedoraproject.org/fedora:44 AS builder

# build dep
RUN \
    dnf -y --nodocs upgrade --refresh \
 && dnf -y --nodocs --setopt=install_weak_deps=False \
      install \
        git git-lfs patch \
        make gcc-c++ cmake libcurl-devel ninja-build \
        hipblas-devel rocm-hip-devel rocblas-devel

# llama.cpp
WORKDIR /tmp/llama.cpp
RUN git clone --recursive https://github.com/ggerganov/llama.cpp.git .

# build
RUN git clean -xdf \
  && git submodule update --recursive \
  && cmake -S . -B build \
      -DCMAKE_INSTALL_PREFIX="/opt/llama.cpp" \
      -DGGML_HIP=ON \
      -DAMDGPU_TARGETS=gfx1151 \
      -DCMAKE_BUILD_TYPE=Release \
      -DGGML_RPC=ON \
  && cmake --build build --config Release -- -j$(nproc) \
  && cmake --install build --config Release

#------------------------------------------------------------------------------------------
# runtime stage
FROM registry.fedoraproject.org/fedora-minimal:44

# runtime deps
RUN \
     microdnf -y --nodocs upgrade --refresh \
  && microdnf -y --nodocs --setopt=install_weak_deps=0 \
       install \
         libatomic libgomp \
         rocm-hip rocblas hipblas rocminfo radeontop procps-ng \
  && microdnf clean all && rm -rf /var/cache/dnf/*

# copy llama.cpp
# COPY --from=builder /opt/llama.cpp /opt/llama.cpp
COPY --from=builder /opt/llama.cpp /usr/local

# ld
#RUN echo "/opt/llama.cpp/lib64" >> /etc/ld.so.conf.d/local.conf \
# && ldconfig
RUN echo "/usr/local/lib64" >> /etc/ld.so.conf.d/local.conf \
 && ldconfig

# helper
COPY gguf-vram-estimator.py /usr/local/bin/gguf-vram-estimator.py
RUN chmod +x /usr/local/bin/gguf-vram-estimator.py

# default run parameter for llama.cpp :
ENV GGML_CUDA_ENABLE_UNIFIED_MEMORY=ON \
    LLAMA_ARG_UBATCH=2048 \
    LLAMA_ARG_FLASH_ATTN=1 \
    LLAMA_ARG_NO_MMAP=1 \
    LLAMA_ARG_N_GPU_LAYERS=999 \
    LLAMA_ARG_MODEL=/models/default.gguf

# default start llama-server
ENTRYPOINT ["/usr/local/bin/llama-server"]
CMD ["--no-warmup"]

